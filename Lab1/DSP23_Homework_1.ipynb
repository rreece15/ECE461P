{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QDMxPNTM-8Ew"
      },
      "source": [
        "## EE 461P: Data Science Principles  \n",
        "### Assignment 1  \n",
        "### Total points: 75\n",
        "### Due: Tuesday, January 31, 2023, submitted via Canvas by 11:59 pm  \n",
        "\n",
        "Your homework should be written in a **Jupyter notebook**. You may work in groups of two if you wish. Only one student per team needs to submit the assignment on Canvas.  But be sure to include name and UT eID for both students.  Homework groups will be created and managed through Canvas, so please do not arbitrarily change your homework group. If you do change, let the TAs know.\n",
        "\n",
        "Also, please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting. (%matplotlib inline)\n",
        "\n",
        "### Name(s) and EID(s):\n",
        "1. Reece Riherd, rdr2793\n",
        "2. (if applicable)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2l4ZpGDS0bVI"
      },
      "source": [
        "# Question 1 (5 points)\n",
        "\n",
        "Read [this](https://www.covid-datascience.com/post/israeli-data-how-can-efficacy-vs-severe-disease-be-strong-when-60-of-hospitalized-are-vaccinated) article that talks about the possibility of vaccines being uneffective in Israel since nearly 60% of severe COVID-19 hospitalization in Israel were vaccinated people. Use Simpson's Paradox to briefly explain if this claim holds true.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qaijrS9q8_go"
      },
      "source": [
        "# Answer 1\n",
        "\n",
        "When applying Simpson's Paradox to the claim that vaccines are ineffective in Israel, the claim is refuted by the reversal of the trend when data among specific age groups is compared. Although vaccines only show 67.5% efficacy when analyzed with every age group, efffectiveness is consistently shown in the range of 88.7% and 100% for most age groups when analyzed individually. This in combination with factors that influence the disparity of severe Covid cases like age disparity, high rates of vaccination, and a higher likelihood to vaccinate in older generations creates confusion in the interpretation of the data. The apparency of the effectiveness of vaccination is diminished by these external factors, but when these factors are separated (such as by age groups), the effectiveness of vaccination is obvious, thus showing a clear example of Simpson's Paradox."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNBbUfy9Fxqg"
      },
      "source": [
        "# Question 2 (10 points)\n",
        "\n",
        "100 students in the previous offering of this class were asked if they wanted to form their own groups for the course project or have the instructor randomize the groups. They reported their preferences by entering Yes or No in the survey. We use 0 to represent preferring their own groups and 1 to represent randomized groups. A random sample of 20 students yielded the following preferences:\n",
        "\n",
        "$$1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1$$\n",
        "\n",
        "These choices are assumed to arise by independent and identically distributed (i.i.d.) sampling from the following distribution and if the unknown parameter $q$ can be estimated, then we can provide more insights about the students' preference regrading the project groups.\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "\\nonumber P(x) = \\left\\{\n",
        "\\begin{array}{l l}\n",
        "    q& \\quad \\text{for  } x=0\\\\\n",
        "1-q & \\quad \\text{for } x=1\n",
        "\\end{array} \\right.\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "Based on the definitions given above, identify the likelihood function and derive the **maximum likelihood estimator** of $q$. Using the given sample, find a maximum likelihood estimate of $q$ as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJ_Kz4VN9Dus"
      },
      "source": [
        "# Answer 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHBVQzeVPs02"
      },
      "source": [
        "# Question 3 (5+5 points)\n",
        "\n",
        "a) Briefly explain what you understand by an estimator of a numeric quantity being unbiased? Show that the MLE for the variance of a Gaussian is biased.\n",
        "\n",
        "b) Suppose the mean of the Gaussian distribution, $\\mu$. So given a data set assumed to be obtained by sampling i.i.d from this Gaussian, your job is to obtain the MLE for the unknown variance. Derive the equation for this estimate and show that it is unbiased."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UovxZfi8Pt7N"
      },
      "source": [
        "# Answer 3\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "a.)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "b.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqV_7myxCupC"
      },
      "source": [
        "# Question 4 (5+5 points)\n",
        "\n",
        "a) What is multicollinearity in the context of linear regression and why is it problematic?\n",
        "\n",
        "b) How do you diagnose and fix multicollinearity?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2a5wCF9C7Vw"
      },
      "source": [
        "# Answer 4"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "a.)\n",
        "Multicollinearity occurs when multiple features of a regression model are correlated with each other, or when those variables are linearly related. This is problematic because multicollinearity makes the results of the model difficult to interpret since they could have been a result of multiple features either individually or together. It can also cause an unstable model that varies significantly given small changes in the data or model, and can result in overfitting, or a decrease in accuracy when applied to another set of data."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "b.)\n",
        "It is possible to detect multicollinearity through the correlation matrix of independent variables. Independent values that are highly correlated with each other in this matrix could be a cause for multicollinearity.\n",
        "\n",
        "Then, to fix the isue of multicollinearity, the variables which more significantly effect the output of the model can be isolated to reduce the effects of multicollinearity. Then, if the problem still persists, variable transformation could reduce the correlation of the features with each other by changing the presentation or range of the data (for example removing the any data taken from the current month)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8M2y37dATTO"
      },
      "source": [
        "# Question 5 : Regression (40 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIeme-FoAWxH"
      },
      "source": [
        "### 5.1 Generate Data (5 points)\n",
        "Generate a synthetic regression dataset using make_regression from [sklearn.datasets](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html) with the following characteristics :\n",
        "n_samples = 20000, n_features = 20, n_informative = 15, n_targets = 1, coef = True and random_state = 42, bias = True, noise = 0.1.\n",
        "\n",
        "**Reading Assignment** : Read about how this data is generated and the affect of the above mentioned parameters on the data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "TYOT8DJcAX7q"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from matplotlib import pyplot\n",
        "from sklearn.datasets import make_regression\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "x,y,coef = make_regression(n_samples=20000, n_features=20, n_informative=15, n_targets=1, bias=True, noise=0.1, coef=True, random_state=42)\n",
        "\n",
        "# pyplot.scatter(x,y)\n",
        "# pyplot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI4XeUtjAYgc"
      },
      "source": [
        "### 5.2 Perform Regression (10 points)\n",
        "a) Divide the above obtained data into a train/test split by using 20% of the data for testing. Then train a linear regression model using Ordinary Least Squares method from [sklearn](https://scikit-learn.org/stable/modules/linear_model.html). \n",
        "\n",
        "b) Evaluate the trained model using Mean Squared Error on both train and test datasets and report the performance. \n",
        "\n",
        "c) Also, print the coefficients and bias obtained after the fit and compare them with the coefficients and bias that were used for generating the data in 5.1. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "0joZZhB_AbR3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# a.)\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=42)\n",
        "# print(len(x_train))\n",
        "# print(len(y_train))\n",
        "\n",
        "# result = sm.OLS(y_train, x_train).fit()\n",
        "# print(result)\n",
        "# print(result.summary())\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(x_train, y_train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean squared error of test: 0.010471\n",
            "Mean squared error of train: 0.009975\n"
          ]
        }
      ],
      "source": [
        "# b.)\n",
        "y_pred = model.predict(x_test)\n",
        "y_pred2 = model.predict(x_train)\n",
        "\n",
        "\n",
        "print(\"Mean squared error of test: %f\" % mean_squared_error(y_test, y_pred))\n",
        "print(\"Mean squared error of train: %f\" % mean_squared_error(y_train, y_pred2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coefficients when generating data: \n",
            " [49.75788258 90.83665798 25.02841944  0.          4.60361603 30.85350704\n",
            "  0.         93.84635499 56.30241747 65.6311587  17.00530636  0.\n",
            " 31.35960057  0.         60.7651503  54.96244433 28.80887553  0.\n",
            " 87.80872023 96.00842932]\n",
            "Coefficients after fitting the data: \n",
            " [ 4.97568103e+01  9.08349608e+01  2.50288380e+01 -5.34098116e-05\n",
            "  4.60368487e+00  3.08548793e+01  9.58574094e-04  9.38474589e+01\n",
            "  5.63027054e+01  6.56306844e+01  1.70049794e+01  1.37272463e-03\n",
            "  3.13598926e+01  7.87077400e-04  6.07646643e+01  5.49635451e+01\n",
            "  2.88097930e+01  1.50933877e-04  8.78089100e+01  9.60072744e+01]\n",
            "Bias when generating the data was True, or 1.\n",
            "Bias after fitting the data was 0.9995602406110253\n"
          ]
        }
      ],
      "source": [
        "# c.)\n",
        "print(\"Coefficients when generating data: \\n\", coef)\n",
        "print(\"Coefficients after fitting the data: \\n\", model.coef_)\n",
        "print(\"Bias when generating the data was True, or 1.\")\n",
        "print(\"Bias after fitting the data was\", model.intercept_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgzF7qSSAbf5"
      },
      "source": [
        "### 5.3 Residuals (5 points)\n",
        "a) Compute the residuals (difference between predicted and original values)  of the trained model on the test data. Compute and show the mean and variance of the residuals. \n",
        "\n",
        "b) Scatter plot the residuals along with true predictions and observe how are the residuals distributed.\n",
        "\n",
        "(BONUS) How do the above observations relate to one of the asumptions behind the MLR model? (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "xujyYrCEAd1x"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean is 0.08195903246928546\n",
            "variance is 0.003754117994819662\n"
          ]
        }
      ],
      "source": [
        "# a.)\n",
        "\n",
        "residuals = y_test - y_pred\n",
        "mean = np.mean(np.abs(residuals))\n",
        "variance = np.var(np.abs(residuals))\n",
        "print(\"mean is\", mean)\n",
        "print(\"variance is\", variance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# b.)\n",
        "# I don't know if this is right? Should we be plotting against x?\n",
        "pyplot.scatter(y_test, y_pred)\n",
        "pyplot.scatter(y_test, residuals)\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bonus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJPQpojJAeAa"
      },
      "source": [
        "### 5.4 Lasso and Ridge Regression (10 points)\n",
        "\n",
        "a) Run Lasso and Ridge regression on the data generated in 5.1 by varying the parameter alpha from 10^-3 to 10^3. For each value of the alpha, store the train error (MSE), test error (MSE), and norm of the coefficient vector using [numpy.linalg.norm](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html).\n",
        "\n",
        "b) Plot the train error, test error and the norm of the coefficient vector with increasing alpha and note what you observe. Use one plot for train error and test error and another for the norm.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDT3C9W7j4QW"
      },
      "outputs": [],
      "source": [
        "# a.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# b.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWbeD15fRXdl"
      },
      "source": [
        "### 5.5 Real-world Regression Problem (10 points)\n",
        "\n",
        "The dataset in the file ecommerce_dataset.csv is for an ecommerce business trying to predict the annual amount spent by each customer. Use Lasso and Ridge regression on the set of independent variables {Average Session Length, Time on App, Time on Website, Length of Membership}  to predict the dependent variable **Yearly Amount Spent**. Vary the value of alpha in the range 10^-6 to 10^5 and do 5-fold cross-validation using sklearn's KFold to find the value of alpha that gives best performance as measured using MSE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snk9yvieSX5d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "QDMxPNTM-8Ew",
        "NNBbUfy9Fxqg",
        "yHBVQzeVPs02"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "e8cfd1dbc86f0b534087d5775aced4d577639a512f2a900367d7563c49234461"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
