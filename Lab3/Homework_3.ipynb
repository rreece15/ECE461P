{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XX8fe1Qo9z7U"
      },
      "source": [
        "## EE 461P: Data Science Principles  \n",
        "### Homework 3 \n",
        "### Total points: 80\n",
        "### Due: Thursday, Mar 7, 2023, submitted via Canvas by 11:59 pm  \n",
        "\n",
        "Your homework should be written in a **Jupyter notebook**. You may work in groups of two if you wish. Only one student per team needs to submit the assignment on Canvas.  But be sure to include name and UT eID for both students.  Homework groups will be created and managed through Canvas, so please do not arbitrarily change your homework group. If you do change, let the TAs know.\n",
        "\n",
        "Also, please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting. (%matplotlib inline)\n",
        "\n",
        "### Name(s) and EID(s):\n",
        "1. Reece Riherd, rdr2793\n",
        "2. \n",
        "\n",
        "### Homework group No.: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h56p0jo_mIvW"
      },
      "source": [
        "# Q1. - Stochastic Gradient Descent (25 pts)\n",
        "1. (**5pts**) Using stochastic gradient descent, derive the coefficent updates for all 4 coefficients of the model (assuming squared error as your loss function): \n",
        "$$ y = w_0 + w_1x_1 + w_2x_1^{2}x_2 + w_3e^{-x_1}+ w_4log(x_3) $$ \n",
        "\n",
        "\n",
        "2. (**20pts**) Write Python code for an SGD solution to the non-linear model$$ y = w_0 + w_1x_1 + w_2x_1^{2}x_2 + w_3e^{-x_1}+ w_4log(x_3) $$   Try to format similarly to scikit-learn's models. The template of the class is given. The init function of the class takes as input the learning_rate, regularization_constant and number of epochs. The fit method must take as input X,y. The _predict_ method takes an X value (optionally, an array of values). \n",
        "\n",
        "  **a**) Use your new gradient descent regression to predict the data given in ```'samples.csv'```, for 15 epochs, using learning rates: ```[0, .0001, .001, .01, 0.1, 1, 10, 100]``` and regularization (ridge regression) parameters: ```[0,10,100]```. (**13pts**)\n",
        "\n",
        "  **b**) Plot MSE and the $w$ parameters as a function of epoch (for 15 epochs) for the two best combinations of learning_rate and regularization for SGD. Here \"best\" means lowest MSE at the end of 15 epochs. (**5pts**)\n",
        "\n",
        "  ```NOTE``` : In this setting there is no validation/test data, and stopping after a pre-determined number of epochs is an example of an \"early stopping\" approch to avoid overfitting.(This approach is not ideal, but is sometimes employed in the absence of a validation dataset).\n",
        "\n",
        "  **c**) Also report the MSE at the end of 15 epochs that you obtained for these two \"best\" combinations. (**2pts**)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQ_yL5u6mQhg"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "class Regression:\n",
        "    \n",
        "    def __init__(self, learning_rate, regularization, n_epoch):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_epoch = n_epoch\n",
        "        self.regularization = regularization\n",
        "        \n",
        "    def sgd(self, gradient):\n",
        "        self.coef # = please fill this to update self.coef using SGD\n",
        "    \n",
        "        \n",
        "    def fit(self, X, y, update_rule='sgd', plot=False):\n",
        "        mse = []\n",
        "        coefs = []\n",
        "        X = self.get_features(X)\n",
        "        for epoch in range(self.n_epoch):\n",
        "            for i in range(X.shape[0]):\n",
        "                # Compute error\n",
        "                   #please fill this\n",
        "                # Compute gradients\n",
        "                    #please fill this\n",
        "               \n",
        "                # Update weights\n",
        "                self.sgd(gradient)\n",
        "\n",
        "            coefs.append(self.coef)\n",
        "            residuals = y - self.linearPredict(X)         \n",
        "            mse.append(np.mean(residuals**2))\n",
        "            \n",
        "        self.lowest_mse = mse[-1]\n",
        "        if plot == True:\n",
        "            plt.figure()\n",
        "            plt.plot(range(self.n_epoch),mse)\n",
        "            plt.xlabel('epoch')\n",
        "            plt.ylabel('MSE')\n",
        "            plt.figure()\n",
        "            coefs = np.array(coefs)\n",
        "            plt.plot(range(self.n_epoch),coefs[:,0],label='w0')\n",
        "            plt.plot(range(self.n_epoch),coefs[:,1],label='w1')\n",
        "            plt.plot(range(self.n_epoch),coefs[:,2],label='w2')\n",
        "            plt.plot(range(self.n_epoch),coefs[:,3],label='w3')\n",
        "            plt.plot(range(self.n_epoch),coefs[:,4],label='w4')\n",
        "            plt.legend()\n",
        "            plt.xlabel('epoch')\n",
        "            plt.ylabel('parameter value')\n",
        "\n",
        "    def get_features(self, X):\n",
        "        '''\n",
        "        this output of this function can be used to compute the gradient in `fit`\n",
        "        '''\n",
        "        x = np.zeros((X.shape[0], 5))\n",
        "        x[:,0] = 1\n",
        "        x[:,1] = (X[:,0])\n",
        "        x[:,2] = (X[:,0]**2)*X[:,1]\n",
        "        x[:,3] = np.exp(-X[:,0])\n",
        "        x[:,4] = np.log(X[:,2])\n",
        "        \n",
        "        return x\n",
        "        \n",
        "    #def linearPredict(self, X):  \n",
        "        #compute dot product of self.coef and X\n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Q6byylamTsZ"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"samples.csv\",index_col=0)\n",
        "X = np.array([df['x1'].values, df['x2'].values,df['x3'].values]).T\n",
        "y = df['y'].values\n",
        "n_epochs = 15\n",
        "learning_rate = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
        "regularization = [0, 10, 100]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iUUkRKgg97W0"
      },
      "source": [
        "# Q2. Neural Network Basics (10 points)\n",
        "\n",
        "2.1 Can a multi-layered neural network with only linear activation functions in all hidden layers be represented as a neural network without any hidden layer? Explain your answer. (5 points)\n",
        "\n",
        "2.2 Suppose you are solving a problem for image datasets using neural networks. You are wondering if you should use neural network with fully-connected layers or a convolutional neural network. Which one do you think is better suited for this task and why? (Read up on convolutional neural networks if we have not covered this in class by due date). (5 points)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUgeghM7-doR"
      },
      "source": [
        "Solution"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2.1\n",
        "If a multi-layered neural network uses only linear activation functions, then the neural network will itself become a linear function and thus become a linear regression model. This happens because a neural network model cannot use stochastic gradient descent to train a model with linear activation functions.\n",
        "\n",
        "\n",
        "2.2\n",
        "I think that the convolutional neural network is better suited for this task. Because convolutional neural network utilize a series of filters, the convolutional neural network will reduce the dimensions of the input, increasing the efficiency of the model and allowing it to handle large amounts of data. This applies especially well to images, since each image can hold multiple thousands of points of data. When requiring multiple images to be processed in a model, convolutional neural networks can handle the amount of data much better than a neural network with fully connected layers and still provide very high accuracy for their classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEwJromXmcBd"
      },
      "source": [
        "# Q3. MLP Regression with Pytorch (30 points)\n",
        "Pytorch provides extensive built-in functions useful for developing and training neural networks. In this exercise, we will try to build a Multi-layer Perceptron for a regression problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEj6Bl8ko6Ru"
      },
      "source": [
        "2.1 Dataset : Download the diabetes dataset from sklearn.datasets. Divide this dataset into train and test sets with 75% of data in train and 25% in test with random_seed = 42. To use this dataset with Pytorch we need to convert it to a pytorch compatible dataset, which is an object of the torch.utils.Dataset class. Use the class below to convert this diabetes dataset to a torch.utils.Dataset object. (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kALK3gBpFFH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvBDy5fEpFiO"
      },
      "outputs": [],
      "source": [
        "class DatasetCustom(torch.utils.data.Dataset):\n",
        "  def __init__(self, X, y):\n",
        "      self.X = torch.from_numpy(X)\n",
        "      self.y = torch.from_numpy(y)\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.X)\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "      return self.X[i], self.y[i]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XG8ps8yVpMo2"
      },
      "source": [
        "2.2 Pytorch uses DataLoader class to load and iterate over a dataset for learning. Create a DataLoader object from the pytorch compatible dataset generated above with batchsize = 64 and shuffle = True. You can read more about Pytorch's Dataset and DataLoader [here](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html). (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2r-hbEohpO0g"
      },
      "source": [
        "2.3 Network Design : Create a python class for a neural network with the following architecture:\n",
        "\n",
        "   a) MLP : Linear layer (input_dim=10, output_dim = 4) followed by a ReLu layer and another linear layer (input_dim=4, output_dim = 1).\n",
        "\n",
        "Pytorch [layers](https://pytorch.org/docs/stable/nn.html) are described here. The individual layers are stacked inside [Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential). \n",
        "\n",
        "Each class implements an init() method that defines the network and a forward method that performs the operations defined under the network. See the example below for a 3-layer neural network. (5 points)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKObwvnZpgBb"
      },
      "outputs": [],
      "source": [
        "class Example(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Linear(784, 64),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(64, 32),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(32, 1)\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95ApP8oUphiv"
      },
      "source": [
        "2.4 Training : Train the above defined neural network using SGD optimizer with learning rate = 0.0001 for 50 epochs while minimizing for the MSELoss. (10 points) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcrfUJv9pkrg"
      },
      "outputs": [],
      "source": [
        "# Initialize the network\n",
        "net = MLP()\n",
        "\n",
        "loss_function = nn.L1Loss()\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.0001)\n",
        "  \n",
        "for epoch in range(0, 500): \n",
        "    print(f'Epoch {epoch+1}')\n",
        "    loss = 0.0\n",
        "    optimizer.zero_grad()\n",
        "    # Iterate over the DataLoader for training data\n",
        "    for i, data in enumerate(trainloader):\n",
        "\n",
        "      inputs, targets = data\n",
        "      # predict for inputs using net\n",
        "      # compute loss for predictions wrt targets\n",
        "      # Call backward on loss\n",
        "      # take one step of optimization\n",
        "      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7llICZOppBu"
      },
      "source": [
        "2.4 Plot the epoch loss (average of losses in all batches in one epoch) vs epoch when you set batch size = 64. In another figure, plot for epoch loss when you set batch size = 1. What do you observe? (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaDhxMM-nqkF"
      },
      "source": [
        "#Question 4: Tensorflow Playground (15 pts)\n",
        "\n",
        "In this question, you will be playing with [Tensorflow Playground](https://playground.tensorflow.org).\n",
        "\n",
        "Select \"**Classification**\" as the **Problem Type**. Among the four datasets shown in DATA, please select the **top right** dataset. \n",
        "\n",
        "Use the following settings as the DEFAULT settings for all **subquestions**: Learning rate = 0.03, Activation = Tanh, Regularization = None, Ratio of training to test data = 50%, Noise = 0, Batch Size = 30, input as $X_1$ with $X_2$, One hidden layer with two neurons, .\n",
        "\n",
        "a) **(4 pts)** Use the DEFAULT setting and run two experiments - one using **Tanh** as the activation function and one using the **Linear** activation function. Report the train, test losses for both at the end of **1000 epochs**. What qualitative difference do you observe in the decision boundaries obtained? What do you think is the reason for this? \n",
        "\n",
        "We will now study the effect of certain variations in the network structure or training process, keeping all other aspects the same as in the DEFAULT setting specified above, with **Tanh** as the activation.\n",
        "\n",
        "b) **(4 pts)** Effect of number of hidden units: Keep other settings the same as in DEFAULT, report the training loss and test loss at the end of 1000 epochs **using 4 neurons and 8 neurons in the hidden layer**. What do you observe in terms of the decision boundary obtained as the number of neurons increases? What do you think is the reason for this? \n",
        "\n",
        "c) **(4 pts)** Effect of Learning rate and number of epochs: Keep other settings the same as in DEFAULT, change the Activation to **ReLU** and use **4 neurons** in the hidden layer. For learning rate 10, 1, 0.1, 0.01 and 0.001, report the train, test losses at the end of **100 epochs**, **1000 epochs** respectively. What do you observe from the change of loss vs learning rate, and the change of loss vs epoch numbers? \n",
        "\n",
        "d) **(3 pts)** Use the DEFAULT setting. Play around with any hyperparameters, network architectures or input features (such as $\\sin(X_1), X_1^2$ etc.), and report the best train and test loss you obtain (test loss should be no greater than 0.06). Attach the screenshot showing your full network, output and the parameters. Briefly justify your results, and comment on what helps/what doesn't help with lowering the loss, etc. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOKaMFKvns2L"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7eabb58302c5fc2ed0886cc51eb11065f6aebf8d2b27d81468f956397d9c6ce"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
