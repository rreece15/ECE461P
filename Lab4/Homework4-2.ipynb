{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzTi6YO2oYFp"
   },
   "source": [
    "## EE 461P: Data Science Principles  \n",
    "### Homework 4 \n",
    "### Total points: 90\n",
    "### Due:  submitted via Canvas by March 30 11:59 pm  \n",
    "\n",
    "Your homework should be written in a **Jupyter notebook**. You may work in groups of two if you wish. Only one student per team needs to submit the assignment on Canvas.  But be sure to include name and UT eID for both students.  Homework groups will be created and managed through Canvas, so please do not arbitrarily change your homework group. If you do change, let the TAs know.\n",
    "\n",
    "Also, please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting. (%matplotlib inline)\n",
    "\n",
    "### Name(s) and EID(s):\n",
    "1. Reece Riherd, rdr2793\n",
    "2. Blake Schwartz, bps826\n",
    "\n",
    "### Homework group No.:50 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "0ApnQfIyoe_N"
   },
   "source": [
    "## Q1. Bayes Optimal Classifier (15 points)\n",
    "Consider a binary classification problem with the following loss matrix -\n",
    "$$\n",
    "   {\\begin{array}{ccccc}\n",
    "   & & \\text{Predicted class} & \\text{           } &\\\\\n",
    "   & & C1 & C2 & Reject\\\\\n",
    "   \\text{True class} & C1 & 0 & 3 & c  \\\\\n",
    "   & C2 & 2 & 0 & c \\\\\n",
    "  \\end{array} } \n",
    "$$\n",
    "where the cost of rejection is a constant. Determine the prediction that minimizes the expected loss in different ranges of $P(C1|x)$ for the following three cases - \n",
    "1. c = 0\n",
    "2. c = 2\n",
    "3. c = 1\n",
    "\n",
    "Hint : Think and work out the bayes classifier as studied in slides aml-classification1.pdf.\n",
    "\n",
    "Answer:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\alpha_1 = Choose \\space C_1 \\\\\n",
    "&\\alpha_2 = Choose \\space C_2 \\\\\n",
    "&\\alpha_r = Reject \\\\\n",
    "% &\\begin{split}\n",
    "R(\\alpha_1|x) & = 0*(P(C_1|x))+3*(P(C_2|x)) \\\\\n",
    "& = 3*(1-P(C_1|x)) \n",
    "% &\\end{split} \\\\\n",
    "&R(\\alpha_2|x)=2*(P(C_1|x)) \\\\\n",
    "&R(\\alpha_r|x) = c \\\\\n",
    "&\\text{Choose C1 if:} \\\\\n",
    "&R(\\alpha_1|x) < c) => P(C_1|x)< \\frac{c}{2} \\\\\n",
    "&\\text{Otherwise, reject C1 and C2} \\\\\\\\\n",
    "&\\text{1) if c = 0:} \\\\\n",
    "&\\text{Choose C1 if P(C1|x) > 1 -> impossible} \\\\\n",
    "&\\text{Choose C1 if P(C1|x) < 0 -> impossible} \\\\\n",
    "&\\text{Therefore, always reject both C1 and C2} \\\\\\\\\n",
    "&\\text{2) if c = 2:}\\\\\n",
    "&\\text{Choose C1 if P(C1|x) > } \\frac{1}{3} \\\\\n",
    "&\\text{Choose C2 if P(C1|x) < 1} \\\\\n",
    "&\\text{Now, we must compare C1 to C2:} \\\\\n",
    "&\\text{When is } R(\\alpha_1|x) < R(\\alpha_2|x)? \\\\\n",
    "% &\\begin{split}\n",
    "3-3*(P(C_1|x)<2*(P(C_1|x)) \\\\\n",
    "3 < 5*(P(C_1|x)) \\\\\n",
    "P(C_1|x) > \\frac{3}{5}\n",
    "% &\\end{split} \\\\ \n",
    "&\\text{Therefore, choose C1 if } P(C_1|x)>\\frac{3}{5} \\\\\n",
    "&\\text{Else, choose C2} \\\\\\\\\n",
    "&\\text{3) if c = 1:} \\\\\n",
    "&\\text{Choose C1 if } P(C_1|x) > \\frac{2}{3} \\\\\n",
    "&\\text{Choose C2 if } P(C_1|x) < \\frac{1}{2} \\\\\n",
    "&\\text{Else, reject both C1 and C2}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AA1U8npSpL3_"
   },
   "source": [
    "## Q2. PCA (25 points)\n",
    "\n",
    "MNIST is a dataset consisting images of digits (0-9). Load the dataset from `mnist.csv`. Then standardise the datasets using StandardScaler from `sklearn.preprocessing` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "eCDShLsLtGzv"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.linalg import eigh\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import eig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "wUkCpLj0qZ2j"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('mnist.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "0CWZM42orwH2"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_dataset = scaler.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVvvGfIardae"
   },
   "source": [
    "2.1 Now, do PCA and get 2 principal components of the data by performing following steps (10 points):\n",
    "  \n",
    "\n",
    "*   Create a covariance matrix of the scaled data\n",
    "*   Obtain the two eigen vectors of the covariance matrix corresponding to the highest two eigen values\n",
    "*   Project the scaled data onto these eigen vectors to obtain a 2-dimesnional representation of the instances in the data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "YT1p_xFxQ4dz"
   },
   "outputs": [],
   "source": [
    "class PCA():\n",
    "  def __init__(self):\n",
    "    self.num_components = None\n",
    "    self.eigen_values = None\n",
    "    self.eigen_vectors = None\n",
    "    self.projection_matrix = None\n",
    "    self.sorted_eigen_values = None\n",
    "    self.explained_variance = None\n",
    "    self.explained_variance_ratio = None\n",
    "\n",
    "  def fit(self, X, k):\n",
    "    ## code for getting the eigen vectors from the data\n",
    "    cov_matrix = np.cov(X)\n",
    "    self.eigen_values,self.eigen_vectors = np.linalg.eig(cov_matrix)\n",
    "    \n",
    "    self.get_principal_components(2)\n",
    "    self.explained_variance = self.eigen_values[self.sorted_eigen_values]\n",
    "    self.explained_variance_ratio = self.explained_variance / self.eigen_values.sum()\n",
    "\n",
    "  def get_principal_components(self, k):\n",
    "    ## code for obtaining eigen vectors corresponding to the top-k eigen values\n",
    "    self.num_components = k\n",
    "    self.sorted_eigen_values = np.argsort(self.eigen_values)[::-1]\n",
    "    self.projection_matrix = self.eigen_vectors[self.sorted_eigen_values[:self.num_components]]\n",
    "\n",
    "  def transform(self, data, k):\n",
    "    ## code for getting projections on the determined principal components\n",
    "    return np.dot(data, self.projection_matrix.T)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "TJhJmiQ4WhXV"
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 26.3 GiB for an array with shape (42000, 42000) and data type complex128",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m instance \u001b[39m=\u001b[39m PCA()\n\u001b[1;32m----> 2\u001b[0m instance\u001b[39m.\u001b[39;49mfit(scaled_dataset, \u001b[39m2\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[39], line 14\u001b[0m, in \u001b[0;36mPCA.fit\u001b[1;34m(self, X, k)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, k):\n\u001b[0;32m     12\u001b[0m   \u001b[39m## code for getting the eigen vectors from the data\u001b[39;00m\n\u001b[0;32m     13\u001b[0m   cov_matrix \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mcov(X)\n\u001b[1;32m---> 14\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meigen_values,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39meigen_vectors \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49meig(cov_matrix)\n\u001b[0;32m     16\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_principal_components(\u001b[39m2\u001b[39m)\n\u001b[0;32m     17\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexplained_variance \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meigen_values[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msorted_eigen_values]\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36meig\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\numpy\\linalg\\linalg.py:1304\u001b[0m, in \u001b[0;36meig\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m   1301\u001b[0m extobj \u001b[39m=\u001b[39m get_linalg_error_extobj(\n\u001b[0;32m   1302\u001b[0m     _raise_linalgerror_eigenvalues_nonconvergence)\n\u001b[0;32m   1303\u001b[0m signature \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mD->DD\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m isComplexType(t) \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39md->DD\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m-> 1304\u001b[0m w, vt \u001b[39m=\u001b[39m _umath_linalg\u001b[39m.\u001b[39;49meig(a, signature\u001b[39m=\u001b[39;49msignature, extobj\u001b[39m=\u001b[39;49mextobj)\n\u001b[0;32m   1306\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m isComplexType(t) \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(w\u001b[39m.\u001b[39mimag \u001b[39m==\u001b[39m \u001b[39m0.0\u001b[39m):\n\u001b[0;32m   1307\u001b[0m     w \u001b[39m=\u001b[39m w\u001b[39m.\u001b[39mreal\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 26.3 GiB for an array with shape (42000, 42000) and data type complex128"
     ]
    }
   ],
   "source": [
    "instance = PCA()\n",
    "instance.fit(scaled_dataset, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (42000,785) and (2,42000) not aligned: 785 (dim 1) != 2 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m projection \u001b[39m=\u001b[39m instance\u001b[39m.\u001b[39;49mtransform(scaled_dataset,\u001b[39m2\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[22], line 28\u001b[0m, in \u001b[0;36mPCA.transform\u001b[1;34m(self, data, k)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransform\u001b[39m(\u001b[39mself\u001b[39m, data, k):\n\u001b[0;32m     27\u001b[0m   \u001b[39m## code for getting projections on the determined principal components\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m   \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49mdot(data, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprojection_matrix)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (42000,785) and (2,42000) not aligned: 785 (dim 1) != 2 (dim 0)"
     ]
    }
   ],
   "source": [
    "projection = instance.transform(scaled_dataset,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96tpFBPHvT34"
   },
   "source": [
    "2.2 Select the points from the train dataset corresponding to class 0 and 7 and plot their 2-D projections as obtained in the previous step (5 points). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "uwNWzA3NuZmw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.4206528 , -0.11406921,  0.01030031, ..., -0.1097105 ,\n",
       "        -0.10948899,  0.01463549],\n",
       "       [-0.11406921,  0.88720037, -0.09474181, ...,  0.11952843,\n",
       "         0.01833755, -0.07966623],\n",
       "       [ 0.01030031, -0.09474181,  0.24042613, ..., -0.02937971,\n",
       "         0.01533043, -0.04160452],\n",
       "       ...,\n",
       "       [-0.1097105 ,  0.11952843, -0.02937971, ...,  0.80423665,\n",
       "        -0.05949555, -0.01960091],\n",
       "       [-0.10948899,  0.01833755,  0.01533043, ..., -0.05949555,\n",
       "         0.69032923,  0.01043066],\n",
       "       [ 0.01463549, -0.07966623, -0.04160452, ..., -0.01960091,\n",
       "         0.01043066,  0.29365506]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cov(scaled_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxvwyyQtxEdH"
   },
   "source": [
    "2.3 Now partition the dataset into a train and test dataset by doing a 75/25 split. Learn the principal components on the train dataset (after scaling) for number of components in [2, 10, 100]. Use the learnt principal components to obtain projections for both train and test dataset (total 3 train/test pairs, 2-D, 10-D and 100-D). Train one LogisticRegression(random_state=4,max_iter=10000) model for each of the reduced dimesnional space (2-D, 10-D and 100-D). Print the classification reports on test dataset for all the models -  this will print the class-wise Precision, Recall and F1 score. More details on classification report can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html). (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQ7rTIhTYJgy"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Y = dataset['label']\n",
    "X = dataset.drop(['label'], axis = 1)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, shuffle=True, random_state=42)\n",
    "\n",
    "scaled_train = scaler.fit_transform(X_train)\n",
    "scaled_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XGMyW8o9QrFN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLuFk7fjnqO4"
   },
   "source": [
    "## Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8UFG3rgpJtB"
   },
   "source": [
    "# Q3 Feature Selection (25 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UBDdgsSqfeG"
   },
   "source": [
    "1. Explain in your own words what forward and backward feature selection methods are, and their pros and cons. (5 points)\n",
    "2. Use the [feature selection](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html) module from sklearn to select features using Decision Trees on the first 20 features of the [breast_cancer dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html). Do forward and backward feature selection and print what features are being selected. Use `n_features='auto'` and see how many features are being selected. (15 points)\n",
    "4. Briefly explain the results you have obtained. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_dbJV7019lg"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data.data[:, :20] # Use the first 20 features\n",
    "y = data.target\n",
    "feature_names = data.feature_names[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78ljTUZEuqGq"
   },
   "source": [
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_STakMvLEF3u"
   },
   "source": [
    "## Q4. Classification using Decision Trees and MLP (25 points)\n",
    "\n",
    "Use the example provided [here](https://scikit-learn.org/stable/modules/tree.html) to get an idea of how to use sklearn's decision tree classifier.\n",
    "\n",
    "1. Load the dataset provided in [`spambase.data`](https://drive.google.com/file/d/1Vl2HOc9UAxJuwXabUmOJc8sQv8F57QfB/view?usp=sharing) and use scikit-learn to create a train-test split as shown.\n",
    "2. Train a decision tree model on the training dataset and report the [confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) and the [ROC-AUC curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) on the test set. (10 points) \n",
    "3. Train an [MLP classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) with the default parameters and report the confusion matrix and ROC-AUC curve on the test set. (10 points)\n",
    "4. Note your observations on the performance of the 2 models. (5 points)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PeII7xlREF3v"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import metrics\n",
    "\n",
    "data = pd.read_csv('spambase.data')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop(columns=['label']), \n",
    "                                                    data[['label']], \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_WbtN9hIhO5"
   },
   "source": [
    "# Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rC0K6xFWIiG2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
