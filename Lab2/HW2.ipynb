{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oXEDGbnisgUk"
      },
      "source": [
        "## EE 461P: Data Science Principles  \n",
        "### Homework 2  \n",
        "### Total points: 95\n",
        "### Due: Feb 16, 2023, submitted via Canvas by 11:59 pm  \n",
        "\n",
        "Your homework should be written in a **Jupyter notebook**. You may work in groups of two if you wish. Only one student per team needs to submit the assignment on Canvas.  But be sure to include name and UT eID for both students.  Homework groups will be created and managed through Canvas, so please do not arbitrarily change your homework group. If you do change, let the TAs know.\n",
        "\n",
        "Also, please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting. (%matplotlib inline)\n",
        "\n",
        "### Name(s) and EID(s):\n",
        "\n",
        "1. rdr2793\n",
        "\n",
        "2 (if applicable)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXdxq_q-spA8"
      },
      "source": [
        "# Q1. Bias-Variance Trade-off (20 points)\n",
        "\n",
        "## 1.1 Bias-Variance Decomposition (10 points)\n",
        "Consider a real-valued function $h(x)$. You were trying to estimate this function using a regression model on the dataset $D$ consisting of $(X, Y)$ pairs. Let the output of the training procedure be another function given by $y(x;D)$. This new function $y(x;D)$ depends on $D$ since it was obtained by regressing on $D$.\n",
        "\n",
        "To evaluate how well $y(x;D)$ generalizes we are interested in computing the expected error $E_D [(y(x; D) - h(x))^2]$, where the expectation is over all datasets of the same size as $D$, each obtained by i.i.d. sampling from the underlying joint distribution of X and Y. Show that this expected error decomposes into a bias term plus a variance term that you have seen in lecture slides named '2A dsp regression1.pdf'.\n",
        "\n",
        "Hint: \n",
        "Write\n",
        "$$\n",
        "  (y(x; D) - h(x))^2 = ( y(x; D) - E_D [y(x; D)] + E_D [y(x; D)] - h(x))^2 = (y(x; D) - E_D [y(x; D)])^2 + (E_D [y(x; D)] - h(x))^2 - 2 (y(x; D) - E_D [y(x; D)]) (E_D [y(x; D)] - h(x))\n",
        "$$\n",
        "\n",
        "And take expectation over $D$ on both sides"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkKQnQVHwfcP"
      },
      "source": [
        "## 1.2 Bias-Variance Explanation (5 points)\n",
        "Briefly explain the bias and variance formulas that were derived above and describe how they relate to underfitting/overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgMK6NQVxHii"
      },
      "source": [
        "## 1.3 Underfitting vs Overfitting (5 points)\n",
        "Suppose you have randomly divided the given dataset $D$ into a training dataset and a test dataset. Now you keep on gradually reducing the size of train dataset by moving some points to the test set. As the train set size decreases, what do you\n",
        "expect will happen to the train and test errors? In your answers, consider both the expected values of these two quantities as well the spread (variance) in the values obtained. Justify briefly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBUZiRbsNCa0"
      },
      "source": [
        "## Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tMml6_T6-4x"
      },
      "source": [
        "# Q2. Log-likelihood (10 points)\n",
        "\n",
        "Consider a linear regression model $y = w.x + ϵ$. Here $x$ is a scalar. The noise  $ϵ$ is IID but depends on $x$ in a way described below:\n",
        "$$\n",
        "ϵ_i ∼ N(0, σ^2) \\quad \\text{if } x_i > 0 \n",
        "$$\n",
        "and   \n",
        "$$\n",
        "ϵ_i ∼ N(0, 4σ^2) \\quad \\text{if } x_i \\leq 0 \n",
        "$$\n",
        "Given $n$ observations ${y_1,y_2, \\dots y_n}$, derive the negative log-likelihood term for this assumed generative model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zR8rgZxjND8H"
      },
      "source": [
        "## Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8at2tjzEnd6e"
      },
      "source": [
        "# Q3. Model Complexity and Bias/Variance Trade-off (35 points)\n",
        "\n",
        "3.1 Load the data given in `all_data_q5.npy` using `numpy.load()` function. This dataset contains the train and test datasets in `(x_train, y_train)` and `(x_test, y_test)` respectively. \n",
        "\n",
        "Now, fit the polynomial models of degrees 1, 5, and 10 on the training data, and print out the mean squared error for train and test datasets for all the models. Essentially, we are trying to fit linear models of this form: $\\hat f(x) = \\beta_0 + \\beta_1x + \\beta_1 x^2 + ... + \\beta_px^p$, where $p$ is the degree of the polynomial. (10 points)\n",
        "\n",
        "Visualise the trained models by making predictions on evenly spaced numbers on x-axis in a fixed range, for eg. you can generate x's by calling `x_all = np.linspace(0, 1, 75).reshape(-1,1)` and call predict on x_all.\n",
        "\n",
        "In the same figure, add the following plots:\n",
        "\n",
        "i) Train data plot : y_train vs x_train\n",
        "\n",
        "ii) Test data plot : y_test vs x_test\n",
        "\n",
        "All the plots must clearly labeled. (10 points)\n",
        "\n",
        "\n",
        "\n",
        "**Tips**: you can use `np.vander(np.squeeze(x_train), deg+1)` to generate the `deg`-degree polynomial vector of `x_train`. For example, `np.vander(np.squeeze(x_train), 3)` gives you the second-degree polynomial of `x_train` and you can call `np.vander` inside the fit method of linear regression. \n",
        "\n",
        "\n",
        "Make use of the starter code we have provided, and fill the `plot_curves` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZR8_J4NttejE"
      },
      "outputs": [],
      "source": [
        "from sklearn import linear_model as lm\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "data_load = np.load('./all_data_q5.npy', allow_pickle=True)\n",
        "x_train = data_load.item().get(\"x_train\")\n",
        "y_train = data_load.item().get(\"y_train\")\n",
        "x_test = data_load.item().get(\"x_test\")\n",
        "y_test = data_load.item().get(\"y_test\")\n",
        "\n",
        "lrp = LinearRegression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJpFXQW0FpDf"
      },
      "outputs": [],
      "source": [
        "def plot_curves(x_train, y_train, x_test, y_test):\n",
        "  # Fit polynomial models of degrees 1, 5, 10 to the training data. \n",
        "  # Print out the mean squared error (on both train and test sets) for all the models. \n",
        "  # Plot the data (y_train vs x_train and y_test vs x_test), the fitted models (predictions on x_all by different models vs x_all), and the predictions on the test set (predictions on x_test by different models vs x_test). \n",
        "\n",
        "  # YOUR CODE COMES HERE\n",
        "  print(\"IMPLEMENT ME!\")\n",
        "  return [0, 0, 0], [0, 0, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpTIjFRy05wJ"
      },
      "source": [
        "Fit the different polynomials to the training data and make the plots:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-e4QCIhzfXr",
        "outputId": "5578dc79-f493-40c6-9745-ac89c126d29b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IMPLEMENT ME!\n"
          ]
        }
      ],
      "source": [
        "train_rmses_100, test_rmses_100 = plot_curves(x_train, y_train, x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxQ6Am5ktoZN"
      },
      "source": [
        "3.2 Which model gives the best performance (measured by MSE)? Explain in terms of the bias-variance tradeoff. (5 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pgrVHp9uM6-"
      },
      "source": [
        "3.3 Analyse how the training data size affects bias and variance of the models. For this, run the analysis in (a) using 20, 40, 60, 80 and all 100 data points. For each of the three models, plot $log(MSE)$ on train dataset vs the size of the training data and again $log(MSE)$ on test dataset vs the size of the training data. State the trends you see as you change the size of the training data on each of the models and explain why you see them. All plots must be in a single figure and labelled correctly. (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foRnHC2P0zC5"
      },
      "source": [
        "Study the effects of the training data size on the bias and variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRXUPkQ6zdix",
        "outputId": "b2f4e1e6-887d-4a82-eb1b-79f29e669361"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20% data\n",
            "IMPLEMENT ME!\n",
            "40% data\n",
            "IMPLEMENT ME!\n",
            "60% data\n",
            "IMPLEMENT ME!\n",
            "80% data\n",
            "IMPLEMENT ME!\n",
            "100% data\n",
            "IMPLEMENT ME!\n"
          ]
        }
      ],
      "source": [
        "print(\"20% data\")\n",
        "train_rmses_20, test_rmses_20 = plot_curves(x_train[40:60], y_train[40:60], x_test, y_test)\n",
        "print(\"40% data\")\n",
        "train_rmses_40, test_rmses_40 = plot_curves(x_train[30:70], y_train[30:70], x_test, y_test)\n",
        "print(\"60% data\")\n",
        "train_rmses_60, test_rmses_60 = plot_curves(x_train[20:80], y_train[20:80], x_test, y_test)\n",
        "print(\"80% data\")\n",
        "train_rmses_80, test_rmses_80 = plot_curves(x_train[10:90], y_train[10:90], x_test, y_test)\n",
        "print(\"100% data\")\n",
        "train_rmses_100, test_rmses_100 = plot_curves(x_train, y_train, x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HogKf84Fxpt"
      },
      "source": [
        "## Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKaVV0ySySKN"
      },
      "source": [
        "# Q4. Stochastic Gradient Descent (20 points)\n",
        "\n",
        "4.1 In class you studied about SGD wherein instead of computing the \"full\" gradient over all data points $\\nabla f(w) = \\dfrac{1}{N} ∑_{i=1}^{N} \\nabla f_i(w)$, you just estimate the gradient based on one randomly selected data point $\\nabla f_i(w)$ at a time, and use it to move forward in optimization. Show that this \"stochastic gradient\" is an unbiased estimator of the full gradient. (10 points)\n",
        "\n",
        "4.2 Suppose you are using SGD optimization to solve a linear regression problem $y = w^Tx$ to obtain the $w$ that minimizes the mean squared error $E(w) = \\dfrac{1}{N} \\sum_{i=1}{N} (y_i - w^Tx_i)^2$ . Given that you initialise $w$ as $w_0$, write the one step update equation for $w$. Here $x$ is a vector with $M+1$ components. (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIeOjV9uNDQg"
      },
      "source": [
        "## Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKvWcCbOyR0E"
      },
      "source": [
        "# Q5. Neural Network Basics (10 points)\n",
        "\n",
        "5.1 Can a multi-layered neural network with only linear activation functions in all hidden layers be represented as a neural network without any hidden layer? Explain your answer. (5 points)\n",
        "\n",
        "5.2 Suppose you are solving a problem for image datasets using neural networks. You are wondering if you should use neural network with fully-connected layers or a convolutional neural network. Which one do you think is better suited for this task and why? (Read up on convolutional neural networks if we have not covered this in class by due date). (5 points)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jK1iR_rtNC4C"
      },
      "source": [
        "## Answer:"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
